{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIRS_tf_template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOM4A1XQPr0MXFnWz9pbQKl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/airsresincrop/AIRS/blob/master/AIRS_tf_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Cuk5h183OZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSw94S_-9FtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number = 2048"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppmdr-kf84Qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp drive/My\\ Drive/data/AIRS_{number}.zip AIRS_{number}.zip\n",
        "!unzip AIRS_{number}.zip\n",
        "!rm AIRS_{number}.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyhCPxmEFHM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unet(input_size = (None, None, 3), lr = 1e-3, loss = 'mean_squared_error', last_act = 'softmax', kern_size = 3, conv_factor = 1, metrics = ['accuracy']):\n",
        "\n",
        "    inputs = layers.Input(input_size)\n",
        "    conv1 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = layers.Conv2D(128 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = layers.Conv2D(128 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = layers.Conv2D(256 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = layers.Conv2D(256 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = layers.Conv2D(512 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = layers.Conv2D(512 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    drop4 = layers.Dropout(0.5)(conv4)\n",
        "    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = layers.Conv2D(1024 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "    conv5 = layers.Conv2D(1024 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "    drop5 = layers.Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = layers.Conv2D(512 // conv_factor, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(layers.UpSampling2D(size = (2,2))(drop5))\n",
        "    merge6 = layers.concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = layers.Conv2D(512 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "    conv6 = layers.Conv2D(512 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "    up7 = layers.Conv2D(256 // conv_factor, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(layers.UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = layers.concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = layers.Conv2D(256 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = layers.Conv2D(256 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    up8 = layers.Conv2D(128 // conv_factor, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(layers.UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = layers.concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = layers.Conv2D(128 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = layers.Conv2D(128 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    up9 = layers.Conv2D(64 // conv_factor, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(layers.UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = layers.concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = layers.Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = layers.Conv2D(2, 1, activation = last_act, dtype=tf.float32)(conv9)\n",
        "\n",
        "    model = models.Model(inputs = inputs, outputs = conv10)\n",
        "\n",
        "    model.compile(optimizer = optimizers.Adam(lr = lr), loss = loss, metrics = metrics)\n",
        "\n",
        "    return model\n",
        "\n",
        "def unet_1(input_size = (None, None, 3), lr = 1e-3, loss = 'mean_squared_error', last_act = 'softmax', kern_size = 3, conv_factor = 1, metrics = ['accuracy']):\n",
        "\n",
        "    inputs = layers.Input(input_size)\n",
        "    conv1 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    \n",
        "    conv2 = layers.Conv2D(128 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = layers.Conv2D(128 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    \n",
        "    conv3 = layers.Conv2D(256 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = layers.Conv2D(256 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    drop3 = layers.Dropout(0.5)(conv3)\n",
        "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(drop3)\n",
        "    \n",
        "    conv4 = layers.Conv2D(512 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = layers.Conv2D(512 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    drop4 = layers.Dropout(0.5)(conv4)\n",
        "\n",
        "    up7 = layers.Conv2D(256 // conv_factor, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(layers.UpSampling2D(size = (2,2))(drop4))\n",
        "    merge7 = layers.concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = layers.Conv2D(256 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = layers.Conv2D(256 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    up8 = layers.Conv2D(128 // conv_factor, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(layers.UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = layers.concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = layers.Conv2D(128 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = layers.Conv2D(128 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    up9 = layers.Conv2D(64 // conv_factor, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(layers.UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = layers.concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = layers.Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = layers.Conv2D(2, 1, activation = last_act, dtype=tf.float32)(conv9)\n",
        "\n",
        "    model = models.Model(inputs = inputs, outputs = conv10)\n",
        "\n",
        "    model.compile(optimizer = optimizers.Adam(lr = lr), loss = loss, metrics = metrics)\n",
        "\n",
        "    return model\n",
        "\n",
        "def unet_2(input_size = (None, None, 3), lr = 1e-3, loss = 'mean_squared_error', last_act = 'softmax', kern_size = 3, conv_factor = 1, metrics = ['accuracy']):\n",
        "\n",
        "    inputs = layers.Input(input_size)\n",
        "    conv1 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    \n",
        "    conv2 = layers.Conv2D(128 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = layers.Conv2D(128 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    drop2 = layers.Dropout(0.5)(conv2)\n",
        "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(drop2)\n",
        "    \n",
        "    conv3 = layers.Conv2D(256 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = layers.Conv2D(256 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    drop3 = layers.Dropout(0.5)(conv3)\n",
        "\n",
        "    up8 = layers.Conv2D(128 // conv_factor, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(layers.UpSampling2D(size = (2,2))(drop3))\n",
        "    merge8 = layers.concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = layers.Conv2D(128 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = layers.Conv2D(128 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    up9 = layers.Conv2D(64 // conv_factor, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(layers.UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = layers.concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = layers.Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = layers.Conv2D(2, 1, activation = last_act, dtype=tf.float32)(conv9)\n",
        "\n",
        "    model = models.Model(inputs = inputs, outputs = conv10)\n",
        "\n",
        "    model.compile(optimizer = optimizers.Adam(lr = lr), loss = loss, metrics = metrics)\n",
        "\n",
        "    return model\n",
        "\n",
        "def unet_3(input_size = (None, None, 3), lr = 1e-3, loss = 'mean_squared_error', last_act = 'softmax', kern_size = 3, conv_factor = 1, metrics = ['accuracy']):\n",
        "\n",
        "    inputs = layers.Input(input_size)\n",
        "    conv1 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    drop1 = layers.Dropout(0.5)(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(drop1)\n",
        "    \n",
        "    conv2 = layers.Conv2D(128 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = layers.Conv2D(128 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    drop2 = layers.Dropout(0.5)(conv2)\n",
        "\n",
        "    up9 = layers.Conv2D(64 // conv_factor, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(layers.UpSampling2D(size = (2,2))(drop2))\n",
        "    merge9 = layers.concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = layers.Conv2D(64 // conv_factor, kern_size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = layers.Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = layers.Conv2D(2, 1, activation = last_act, dtype=tf.float32)(conv9)\n",
        "\n",
        "    model = models.Model(inputs = inputs, outputs = conv10)\n",
        "\n",
        "    model.compile(optimizer = optimizers.Adam(lr = lr), loss = loss, metrics = metrics)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz9tWlYh9Pdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from random import shuffle\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import pickle\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, Callback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAlniJdC-3VU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_input(filepath):\n",
        "  image = plt.imread(filepath)\n",
        "  return image\n",
        "\n",
        "def get_output(filepath):\n",
        "  label = plt.imread(filepath)\n",
        "  return label\n",
        "\n",
        "def preprocess_input(image):    \n",
        "  return image/255\n",
        "\n",
        "def preprocess_output(label):\n",
        "  label = label[:,:,0].copy()\n",
        "  # label[label==2] = 1\n",
        "  label = np.stack([1-label,label],axis=-1)\n",
        "  return label\n",
        "\n",
        "def generator(files, batch_size=8):\n",
        "  gen = iter(itertools.cycle(files))\n",
        "  while True:\n",
        "    yield [next(gen) for _ in range(batch_size)]\n",
        "\n",
        "def image_generator(dataset, split = 'train', batch_size = 8):\n",
        "  image_files = sorted([f.split('.')[0] for f in os.listdir(f'{dataset}/{split}/images') if 'jpg' in f])\n",
        "  label_files = sorted([f.split('.')[0] for f in os.listdir(f'{dataset}/{split}/labels') if 'tif' in f])\n",
        "  assert image_files == label_files\n",
        "  files = image_files\n",
        "  gen = generator(files, batch_size=batch_size)\n",
        "  while True:\n",
        "    batch_paths = next(gen)\n",
        "    batch_input = []\n",
        "    batch_output = []\n",
        "\n",
        "    for input_path in batch_paths:\n",
        "      inputs = get_input(filepath = f'{dataset}/{split}/images/{input_path}.jpg')\n",
        "      output = get_output(filepath = f'{dataset}/{split}/labels/{input_path}.tif')\n",
        "      inputs = preprocess_input(image = inputs)\n",
        "      output = preprocess_output(label = output)\n",
        "      batch_input += [inputs]\n",
        "      batch_output += [output]\n",
        "\n",
        "    batch_x = np.array(batch_input)\n",
        "    batch_y = np.array(batch_output)\n",
        "\n",
        "    yield (batch_x, batch_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m4v1DnOA0UI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = f'/content/full_data/AIRS_{number}/crop_full'\n",
        "batch_size = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98MA7X1aJKr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = plt.imread(dataset+'/train/labels/0.tif')[:,:,0].copy()\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "for i in range(2):\n",
        "  plt.subplot(1,2,i+1)\n",
        "  plt.imshow((img==i).astype(np.uint8)*255)\n",
        "  plt.axis('off')\n",
        "  plt.tight_layout()\n",
        "fig.patch.set_facecolor('white')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axBsuGlWFjVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for split in ['train', 'val', 'test']:\n",
        "  for label in ['images', 'labels']:\n",
        "    length = len([f for f in os.listdir(f'{dataset}/{split}/{label}') if 'jpg' in f or 'tif' in f])\n",
        "    print(f'{split}_{label}: {length}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jktVpaDpA8Hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "white_vals = []\n",
        "names = [f.split('.')[0] for f in sorted(os.listdir(f'{dataset}/test/labels/'))]\n",
        "for label in names:\n",
        "    white_vals.append(np.sum(plt.imread(f'{dataset}/test/labels/{label}.tif')[:,:,0]))\n",
        "min_idx, max_idx, med_idx = np.argmin(white_vals), np.argmax(white_vals), np.argsort(white_vals)[len(white_vals)//2]\n",
        "fig = plt.figure(figsize=(20,20))\n",
        "for i, idx in enumerate([max_idx, med_idx, min_idx]):\n",
        "  plt.subplot(1,3,i+1)\n",
        "  plt.imshow(plt.imread(f'{dataset}/test/images/{names[idx]}.jpg'), cmap='gray')\n",
        "  plt.imshow(plt.imread(f'{dataset}/test/labels/{names[idx]}.tif')*255, cmap='gray',alpha=0.5)\n",
        "  plt.axis('off')\n",
        "  plt.tight_layout()\n",
        "fig.patch.set_facecolor('black')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-jIPtqdC7DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for conv_factor in [1,2,4,8,16,32,64]:\n",
        "#   print(f'Conv Factor: {conv_factor}')\n",
        "#   for model_func in [unet, unet_1, unet_2, unet_3]:\n",
        "#     modelname = str(model_func).split()[1]\n",
        "#     print(f'{modelname}: {model_func(conv_factor=conv_factor).count_params()}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTyvy9ytEA-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen = image_generator(dataset = dataset, split = 'train', batch_size = batch_size)\n",
        "val_gen = image_generator(dataset = dataset, split = 'val', batch_size = batch_size)\n",
        "test_gen = image_generator(dataset = dataset, split = 'test', batch_size = batch_size)\n",
        "\n",
        "train_spe = len(os.listdir(f'{dataset}/train/images')) // batch_size\n",
        "val_spe = len(os.listdir(f'{dataset}/val/images')) // batch_size\n",
        "test_spe = len(os.listdir(f'{dataset}/test/images')) // batch_size\n",
        "print(train_spe, val_spe, test_spe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ8XWNBYF8YH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def soft_dice_loss(y_true, y_pred): \n",
        "    # skip the batch and class axis for calculating Dice score\n",
        "    axes = tuple(range(1, len(y_pred.shape)-1))\n",
        "    numerator = 2. * K.sum(y_pred * y_true, axes)\n",
        "    denominator = K.sum(K.square(y_pred) + K.square(y_true), axes)\n",
        "    return 1 - K.mean(numerator / (denominator + 1e-6))\n",
        "\n",
        "def soft_dice_metric(y_true, y_pred):\n",
        "    K_quant_preds = K.cast(K.argmin(y_pred, axis = -1),'float32')\n",
        "    K_quant_preds = K.stack([K_quant_preds, 1- K_quant_preds],axis = -1)\n",
        "    K_intersection = K.cast(K.all(K.stack([K_quant_preds, y_true], axis=0), axis=0),'int64')\n",
        "    K_union = K.cast(K.any(K.stack([K_quant_preds, y_true], axis=0), axis=0),'int64')\n",
        "    return (K.sum(K_intersection[:,:,:,1])/K.sum(K_union[:,:,:,1]) + K.sum(K_intersection[:,:,:,0])/K.sum(K_union[:,:,:,0]))/2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xln3iJ1Xf5_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x,y = next(train_gen)\n",
        "# fig = plt.figure(figsize=(30,30))\n",
        "# for i in range(0,16,2):\n",
        "#   plt.subplot(4,4,i+1)\n",
        "#   plt.imshow(x[i//2])\n",
        "#   plt.imshow(y[i//2,:,:,0],alpha=0.5,cmap='gray')\n",
        "#   plt.axis('off')\n",
        "#   plt.tight_layout()\n",
        "#   plt.subplot(4,4,i+2)\n",
        "#   plt.imshow(x[i//2])\n",
        "#   plt.imshow(y[i//2,:,:,1],alpha=0.5,cmap='gray')\n",
        "#   plt.axis('off')\n",
        "#   plt.tight_layout()\n",
        "# fig.patch.set_facecolor('black')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un0cotJbO98N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_act = 'softmax'\n",
        "loss_func = 'dice'\n",
        "loss = soft_dice_loss\n",
        "metric_func = 'dice'\n",
        "metrics = [soft_dice_metric]\n",
        "modelname = 'unet_1'\n",
        "dataset_name = '_'.join(dataset.split('/')[3:])\n",
        "project_name = f'{dataset_name}_{modelname}_vary_cf_4to32'\n",
        "max_attempts = 3\n",
        "\n",
        "for conv_factor in [32,16,8,4]:\n",
        "  vary = conv_factor\n",
        "  print(f'Conv Factor: {conv_factor}')\n",
        "  test_miou = 0\n",
        "  attempts = 0\n",
        "  #while test_miou < 0.5 and attempts < max_attempts:\n",
        "  while attempts < max_attempts:\n",
        "    print(f'Attempt #{attempts}')\n",
        "    project_path = f'results/models/{project_name}/{vary}/{attempts}'\n",
        "    Path(project_path).mkdir(parents=True, exist_ok=True)\n",
        "    print(f'Models saved to {project_path}')\n",
        "    monitor = 'val_soft_dice_metric'\n",
        "    mntr = ''.join([first[0] for first in 'val_soft_dice_metric'.split('_')])\n",
        "    weights_name = f'{modelname}_{last_act}_l:{loss_func}_m:{metric_func}_mntr:{mntr}_cf:{conv_factor}_og_{attempts}'\n",
        "    #ckptname = f'{project_path}/{weights_name}'+'_epochs:{epoch:03d}_mIoU:{val_soft_dice_metric:.4f}.h5'\n",
        "    ckptname = f'{project_path}/{weights_name}.h5'\n",
        "    print('Checkpoint format: ' + ckptname)\n",
        "\n",
        "    checkpointer = ModelCheckpoint(ckptname, monitor = monitor, mode = 'max', save_best_only = True, verbose = 0)\n",
        "    earlystopper = EarlyStopping(monitor = monitor, mode = 'max', patience = 10, verbose = 0)\n",
        "    reduceLR = ReduceLROnPlateau(monitor = monitor, mode = 'max', factor = 1/np.sqrt(10), patience = 5, cooldown = 1 ,verbose = 0)\n",
        "    callbacks = [checkpointer, earlystopper, reduceLR]\n",
        "\n",
        "    epochs = 100\n",
        "    model = unet_1(lr = 1e-2, loss = loss, last_act = last_act, conv_factor = conv_factor, metrics = metrics)\n",
        "    history = model.fit(x = train_gen, steps_per_epoch = train_spe, epochs = epochs, validation_data = val_gen, validation_steps = val_spe, callbacks=callbacks, verbose = 2)\n",
        "\n",
        "    history_path = f'results/hists/{project_name}/{vary}'\n",
        "    Path(history_path).mkdir(parents = True, exist_ok = True)\n",
        "    with open(f'{history_path}/hist_{weights_name}', 'wb') as file_pi:\n",
        "        pickle.dump(history.history, file_pi)\n",
        "    print(f'Saved model history at {history_path}/hist_{weights_name}')\n",
        "\n",
        "    #all_weights = sorted([f for f in os.listdir(project_path) if weights_name in f], key = lambda x: int(x[-18:-15]))\n",
        "    #best_weights = project_path+'/'+all_weights[-1]\n",
        "    #other_weights = all_weights[:-1]\n",
        "    #[os.remove(project_path+'/'+f) for f in other_weights]\n",
        "    best_weights = project_path + '/' + [f for f in os.listdir(project_path) if weights_name in f][0]\n",
        "    model.load_weights(best_weights)\n",
        "    print('Weights loaded from ' + best_weights)\n",
        "\n",
        "    print('Evaluation on test set:')\n",
        "    test_loss, test_miou = model.evaluate(x = test_gen, steps = test_spe, verbose = 0)\n",
        "    print(f'Test Loss: {test_loss:.4f}\\nTest mIoU: {test_miou:.4f}')\n",
        "    attempts += 1\n",
        "    if test_miou < 0.5:\n",
        "      print('Test mIoU too less, retrying...')\n",
        "      continue\n",
        "    miou_path = f'results/mious/{project_name}/{vary}'\n",
        "    Path(miou_path).mkdir(parents = True, exist_ok = True)\n",
        "    val_loss, val_miou = model.evaluate(x = val_gen, steps = val_spe, verbose = 0)\n",
        "    with open(f'{miou_path}/miou_{weights_name}.txt', 'w') as miou_file:\n",
        "      miou_file.write(f'{val_miou:.4f}\\n')\n",
        "      miou_file.write(f'{test_miou:.4f}\\n')\n",
        "    print(f'Saved model mIoUs at {miou_path}/miou_{weights_name}')\n",
        "    max_img = plt.imread(f'{dataset}/test/images/{names[max_idx]}.jpg') / 255\n",
        "    med_img = plt.imread(f'{dataset}/test/images/{names[med_idx]}.jpg') / 255\n",
        "    min_img = plt.imread(f'{dataset}/test/images/{names[min_idx]}.jpg') / 255\n",
        "    max_roof, med_roof, min_roof, _ = model.predict(np.stack([max_img, med_img, min_img, min_img]))\n",
        "    max_roof = max_roof.argmax(-1)\n",
        "    med_roof = med_roof.argmax(-1)\n",
        "    min_roof = min_roof.argmax(-1)\n",
        "    for m in ['max','med','min']:\n",
        "        Path(f'results/outputs/{m}/{project_name}/{vary}').mkdir(parents = True, exist_ok = True)\n",
        "    plt.imsave(f'results/outputs/max/{project_name}/{vary}/max_{weights_name}.jpg', max_roof, cmap='gray')\n",
        "    plt.imsave(f'results/outputs/med/{project_name}/{vary}/med_{weights_name}.jpg', med_roof, cmap='gray')\n",
        "    plt.imsave(f'results/outputs/min/{project_name}/{vary}/min_{weights_name}.jpg', min_roof, cmap='gray')\n",
        "    print(f'Test outputs saved as results/outputs/***/{project_name}/***_{weights_name}.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udFmPg-oYxh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r results_{dataset_name}.zip results/\n",
        "!cp results_{dataset_name}.zip drive/My\\ Drive/keras_models/results_{dataset_name}.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vLN8eLjdFsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEW0bJyVdFpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMlAi64rdFnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYso6StAPWkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "shutil.rmtree('results')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0CY9xjUYbGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}